{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层和块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0559,  0.2158,  0.1552, -0.1478,  0.2858,  0.0304,  0.2131, -0.2499,\n",
       "          0.1447, -0.3082],\n",
       "        [-0.1454,  0.2696,  0.1304, -0.1534,  0.3438, -0.1335,  0.2757, -0.0790,\n",
       "          0.1838, -0.2413]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # 此处用F.relu是因为relu是 激活函数\n",
    "        # 不含参数 因此不用单独记为一层\n",
    "        # nn.ReLU是一个类 而relu是一个函数\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2382, -0.0882,  0.0009,  0.1618,  0.0275,  0.0590,  0.0688,  0.1739,\n",
       "         -0.2814,  0.0353],\n",
       "        [ 0.2753, -0.2081,  0.0754,  0.0461,  0.0655,  0.1305,  0.0863,  0.2872,\n",
       "         -0.2564, -0.1132]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "# 此处通过__call()__函数进行调用 即是一个可调用对象\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential是如何实现的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict()\n",
      "Linear(in_features=20, out_features=256, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "Linear(in_features=20, out_features=256, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=256, out_features=10, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0900, -0.2613, -0.1118, -0.0734,  0.1978,  0.2938,  0.0646, -0.0612,\n",
       "          0.0968, -0.0365],\n",
       "        [ 0.0864, -0.3624, -0.1294, -0.1335,  0.2767,  0.2676,  0.0715, -0.2268,\n",
       "          0.2318,  0.0090]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        print(self._modules)\n",
    "        for block in args:\n",
    "            print(block)\n",
    "            self._modules[block] = block\n",
    "            \n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            print(block)\n",
    "            X = block(X)\n",
    "        return X\n",
    "    \n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在正向传播函数中执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20,20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20,20)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum()>1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0089, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0215,  0.1072,  0.3155,  0.1401, -0.1343, -0.2321,  0.0727,  0.2232,\n",
       "         -0.2747, -0.1684],\n",
       "        [ 0.1457, -0.2081,  0.3325,  0.2457, -0.0579, -0.1076,  0.2100,  0.5497,\n",
       "         -0.2181, -0.1995]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential2(nn.Module):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.seqlist = []\n",
    "        for block in args:\n",
    "            self.seqlist.append(block)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for block in self.seqlist:\n",
    "            X = block(X)\n",
    "        return X\n",
    "net = MySequential2(nn.Linear(20,256), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1946, -0.0354, -0.0998,  0.0151, -0.3166, -0.0090,  0.0552, -0.2227,\n",
       "          0.0759, -0.1213],\n",
       "        [ 0.1356, -0.0145, -0.1149,  0.0410, -0.3505,  0.0749,  0.0423, -0.1783,\n",
       "          0.1479, -0.1061]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NET1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(20,128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear2(self.linear1(X))\n",
    "\n",
    "class NET2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(64, 32)\n",
    "        self.linear2 = nn.Linear(32, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear2(F.relu(self.linear1(X)))\n",
    "\n",
    "class NET3(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.net2(self.net1(X))\n",
    "    \n",
    "net1 = NET1()\n",
    "net2 = NET2()\n",
    "# net2(net1(X))\n",
    "net3 = NET3(net1, net2)\n",
    "net3(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
